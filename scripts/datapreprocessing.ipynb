{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_test_split(raw_data, annotated_data, train_percent=0.8, validation_percent=0.1, test_percent=0.1):\n",
    "  assert train_percent + validation_percent + test_percent == 1.0\n",
    "  raw_files = os.listdir(raw_data)\n",
    "  annotated_files = os.listdir(annotated_data)\n",
    "  assert len(raw_files) == len(annotated_files)\n",
    "  num_files = len(raw_files)\n",
    "\n",
    "  filename_range = [int(f.split('.')[0]) for f in raw_files]\n",
    "  # Adjust indices to match the range of filenames\n",
    "  start_index = min(filename_range)\n",
    "  end_index = max(filename_range)\n",
    "  all_indices = np.arange(start_index, end_index + 1)\n",
    "  \n",
    "  train_size = int(num_files * train_percent)\n",
    "  validation_size = int(num_files * validation_percent)\n",
    "  test_size = num_files - train_size - validation_size\n",
    "  \n",
    "  train_indices = np.random.choice(all_indices, size=train_size, replace=False)\n",
    "  remaining_indices = np.setdiff1d(all_indices, train_indices)\n",
    "  validation_indices = np.random.choice(remaining_indices, size=validation_size, replace=False)\n",
    "  test_indices = np.setdiff1d(remaining_indices, validation_indices)\n",
    "  \n",
    "  # This is for when files are not 0 indexed\n",
    "  # all_indices = np.arange(1, num_files + 1)\n",
    "    \n",
    "  # train_indices = np.random.choice(all_indices, size=int(num_files * train_percent), replace=False)\n",
    "  # remaining_indices = np.setdiff1d(all_indices, train_indices)\n",
    "  # validation_indices = np.random.choice(remaining_indices, size=int(num_files * validation_percent), replace=False)\n",
    "  # test_indices = np.setdiff1d(remaining_indices, validation_indices)\n",
    "\n",
    "  # train_indices = np.random.choice(num_files, size=int(num_files * train_percent), replace=False)\n",
    "  # remaining_indices = np.setdiff1d(np.arange(num_files), train_indices)\n",
    "  # validation_indices = np.random.choice(remaining_indices, size=int(num_files * validation_percent), replace=False)\n",
    "  # test_indices = np.setdiff1d(remaining_indices, validation_indices)\n",
    "\n",
    "\n",
    "\n",
    "  # print(f\"Train indices: {train_indices}\")\n",
    "  # print(f\"Validation indices: {validation_indices}\")\n",
    "  # print(f\"Test indices: {test_indices}\")\n",
    "  \n",
    "  for raw, ann in zip(sorted(raw_files), sorted(annotated_files)):\n",
    "    raw_index = int(raw.split('.')[0])\n",
    "    ann_index = int(ann.split('.')[0])\n",
    "    assert raw_index == ann_index\n",
    "    if (raw_index not in train_indices) and (raw_index not in validation_indices) and (raw_index not in test_indices):\n",
    "      print(f\"Index {raw_index} not found in any set\")\n",
    "      raise ValueError('Index not found in any set')\n",
    "    \n",
    "  return train_indices, validation_indices, test_indices\n",
    "\n",
    "data_root_dir = '/Data/dataLIDAR/0221-1817_seed_9876/clear_night'\n",
    "raw_data_dir = os.path.join(data_root_dir, 'rgb')\n",
    "annotated_data_dir = os.path.join(data_root_dir, 'rgb_seg')\n",
    "train, validate, test = train_validation_test_split(raw_data_dir, annotated_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_odgt(raw_data, annotated_data, train_idx, validate_idx, test_idx, out_dir):\n",
    "  raw_files = os.listdir(raw_data)\n",
    "  annotated_files = os.listdir(annotated_data)\n",
    "  assert len(raw_files) == len(annotated_files)\n",
    "  \n",
    "  # Create output directory if it does not exist\n",
    "  if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "  \n",
    "  for raw, ann in zip(sorted(raw_files), sorted(annotated_files)):\n",
    "    raw_index = int(raw.split('.')[0])\n",
    "    ann_index = int(ann.split('.')[0])\n",
    "    assert raw_index == ann_index\n",
    "\n",
    "    raw_path = os.path.join(raw_data, raw)\n",
    "    ann_path = os.path.join(annotated_data, ann)\n",
    "    raw_img = Image.open(raw_path)\n",
    "    ann_img = Image.open(ann_path)\n",
    "    width, height = raw_img.size\n",
    "    ann_width, ann_height = ann_img.size\n",
    "    assert width == ann_width\n",
    "    assert height == ann_height\n",
    "    \n",
    "    #change paths to be enclosed in double quotes instead of single quotes\n",
    "    odgt_line = {\"fpath_img\": raw_path, \"fpath_segm\": ann_path, \"width\": width, \"height\": height}\n",
    "    if raw_index in train_idx:\n",
    "      with open(os.path.join(out_dir, 'train.odgt'), 'a', encoding='utf-8') as f:\n",
    "          f.write(json.dumps(odgt_line) + '\\n')\n",
    "    elif raw_index in validate_idx:\n",
    "        with open(os.path.join(out_dir, 'validate.odgt'), 'a', encoding='utf-8') as f:\n",
    "          f.write(json.dumps(odgt_line) + '\\n')\n",
    "    elif raw_index in test_idx:\n",
    "        with open(os.path.join(out_dir, 'test.odgt'), 'a', encoding='utf-8') as f:\n",
    "          f.write(json.dumps(odgt_line) + '\\n')\n",
    "    \n",
    "make_odgt(raw_data_dir, annotated_data_dir, train, validate, test, 'odgt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv4ad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
